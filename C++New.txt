++a increment & use. a++ use & increment (a temporary is created to hold & use the same value while a is incremented). So, ++a is faster & better practice 
++a {a = a + 1;return a;}
a++{b = a;a = a + 1;return b;}

Program crash before main() would because Static & Global Variable Initializations.

strcpy stops when encounters a NUL ('\0') character (so does printf), memcpy (&dest,&src,size_t) does not, it binary copy of block size data
    char s[5] = { 's', 'a', '\0', 'c', 'h' };
    char p[5];
    char t[5];
    strcpy(p, s); //p { 's', 'a'}
    memcpy(t, s, sizeof(s)); //t { 's', 'a', '\0', 'c', 'h' }

const O o [=a]; //error [ok] const must be init at declare
O& o [=a]; //error [ok] reference is const pointer (const address value must be init at declare) can not be nullptr
o=b; //ok, same as a=b; copy value b to a, modify object not pointer o still bind to address of a
O* const po [= &a]; //error [ok] 
po=&b;//error, modify pointer
po->m=0;//ok

const O* po = &a; //pointer to const obj
po=&b; //ok
po->m=0; //error, modify object 

const O& o = a; //const ptr to const obj, 
o=b; //error modify object 
const O* const po = &a; //error on both modify pointer and object

Usually use [const] ref as input [readonly] para (pass in ref : init from caller will stay, and return copy: init by func will dealloc), Use ptr (smart) as class member

C++11 constexpr apply const on utility function /constructor by compile-time evaluation as long as not depend on runtime data, safer and efficient 

constexpr int sq(int n) {
    return n*n
}

class Point {
public:
    constexpr Point(int x, int y) : x(x), y(y) {}
private:
    int x, y;
};
//static or global objects 
constexpr Point origin(0, 0);


C++17 std::optional<T> nullable (smart pointer than raw) input or return, .has_value() to check; .value_or(default) to replace std::nullopt and avoid deref nullptr z

= delete means that the compiler will not generate the constructor
class X {//uncopyable 
    X& operator=(const X&) = delete; 
    X(const X&) = delete;

= default use compiler generated 
    X ()=default | {}  //non|user-defined 
    
safe practice prevent unintended implicit conversion by constructor with single para input type to X x=1, which covert 1 to an instance of X by the implicit constructor then call the [default] copy constructor 
    explicit X(int i){} //X x=1 compile error: no viable conversion from 'int' to 'X'; ok X x(1.0)
    [X(const X& c);]
    X(double) =delete; //prevent X x(1.0)
};

C-style (only unsafe on pointer/reference) compile time
int a = 7, b = 2;
double c = (double) a / b; //output (3.5) 3
for std::string s: double c=std::stod(s); //stoi stof stol stoll by C++11

C++
convert to safe type at compile time
double numDouble = static_cast<double>(numInt); 

downcast base ptr to derives ptr at runtime cost (only on polymorphic class pointer/reference because it uses that info to decide safe), return nullptr if fails 
B* ptrB = new D(); //implicit upcast to base
func(B* ptrB){
//ptrB->virtualFunc() uses derived logic, cast to ptrD to access its nonvirtual func  
D* ptrD = dynamic_cast<D*>(ptrB);}

Avoid below cast due to vulnerability 
temp remove const to a pointer/reference so it can be modified or passed to a function not expects const arg 
const int num = 5; 
const int* constPtr= &num; //ptr to const
[const int& constRef=5;
constRef=10;//compile error]
*constPtr =10; //compile error
 int* nonConstPtr = const_cast<int*>(constPtr); 
[int& nonConstRef = const_cast<int&>(constRef);
nonConstRef =10; //ok]
*nonConstPtr = 10; //ok

convert the pointer to any other type of pointer without checking type at compile time
 int num = 10; 
 int* numPtr = &num; 
 char* charPtr= reinterpret_cast<char*>(numPtr); 


class B final
{virtual void func(int);
class D : public B //compile err if above final
{virtual void func(double) override final; 
//compile err if override because sig mismatch, otherwise init a new virtual 
class S : public D
{void func(double); //compile err if above final

//only allocated on heap not stack
class A 
{ 
public: 
A(){cout<<"cstr"<<endl;} //need not ; after {}
void del(){delete this;}
private: 
~A(){cout<<"dstr"<<endl;}
}; 

int main() {
    //A a; //compile err local obj out scope unable private dstr  
    A* a = new A(); // 
 call public cstr 
    std::cout << a;
    //delete a; //same error
    a->del();
}

class Security{
private| protected:
    int settleDate;
   void setSettleDate(int);
public:
    int getSettleDate();
    friend class Order;//access private|protected members
    friend class SecurityTestFixture;
};
class Order{
private:
    Security sec;
public:
    Place(int tradeDate){sec.setSettleDate(tradeDate +1);}
};

foo(){
A* a = new A();
goo(); //if exception, a won’t delete 
delete a;
}

string literal is immutable char *p = "abc"; p[1]=d; won’t change  in read only mem like constant 
std::string is mutable unless constant (unlike Java/c# string from pool, immutable for thread safety in case other threads modify the same instance)

//in|output pointer of func pass a copy of its value (address)
int* p(int* ptr| int arr[], int n) 
{n = sizeof(arr) / sizeof(arr[0]|*arr);//arr is ptr to 1st element *arr++ is arr[1]
int* dup =new int[n];//in heap, int a[n]; in stack
for (int i = 0; i < n; i++) 
        dup[i]=ptr[i] | arr[i] ;
return dup;
}//dup is out  but its value (address to int[] heap memory remain) is copied to r, no dangling nor memory leak

int a[] = { 10, 20, 30 }; 
int* r=p(a,3);
cout<<r[2];
if(r) delete[] r; //release new int[] heap memory 

A a[] = {A("1"), A("2")}; //constructor A(string d),  compile error by A a[2]= {"1","2"}; 

tuple<bool,int,string> t=make_tuple(true,2,”test”);
std:get<1>(t); //2 <tuple>

Each element is a bit (1/0) < char (8bits|1byte)
  bitset<8> foo;//00000000 <2^n>
  bitset<8> bar (4);//00000100, binary value 
  bitset<8> baz (string("01001"));//00001001
  foo.set(3,1);//(3) 00001000 index from right not binary value, boundary check out_of_range exp if i >= size; .set() all to 1; .reset(i)|() [i]|all to 0; . flip(i)|() [i]|all to opposite 
  foo[2]=foo[3]; //00001100, [] no boundary check
  for (std::size_t i=0; i<foo.size(); ++i){
    if(foo.test(i))//& boundary check
        cout<<i<<endl;//2 3, foo.count() is 2
  }
//foo.all|any|none() if all|any|none is 1
Find a missing 013| duplicate number 01223in 0-N: N(N+1)/2 - all elements = missing |-duplicate;
Find a replaced number 0122: bit_set<2^n=N> bs; bs.set(each element); if !bs.test(each element)// if !bs.all()

std::set<std::string> myset;
pair<set<int>::iterator,bool> ret = myset.emplace("foo");
if (!ret.second) {//foo already exists 

myset.erase ("foo");
it = myset.find ("foo");
myset.erase (it[, myset.end()]);

map.at(“k”) = v; override existing value, throws out_of_range if “k”is not an existing key.
map[‘k’] create a new k-default value if not existing by even read access
map.find(“k”) returns an iterator->second point to the value and stop searching after found 
map.count(“k”) returns an int and continue searching till the end

vector.at(i) throw out_of_range | vector[i] nothrow but undefined behavior if i over boundary 

vector dynamically allocate in heap auto delete vs array is fixed size manual delete if in heap

resize(n) change size not capacity, the excess objects under current size are destructed or default-initialized objects are added to the end if over current size
shrink_to_fit() set capacity = size by discard unused allocation. after done building a vector and will never add another item to it. 
reserve(n) If know in advance how many items will be adding, to avoid reallocate changes capacity during growing 

begin|end() returns an iterator (pointer) to first|last element, front|back() returns a direct reference.

for (vector<E|E*>::const_iterator it = ves.begin(); it != ves.end(); ++it)
{it-> or *it.f() | (*it)->f();} //*it is obj | ptr


N threads process runs on M cores CPU, N<=M OS assign each thread a core to run parallel, N>M threads share core by splitting processor time and context switching from thread to thread
thread can share an address space with other threads (but own register and stack for locals), such as global variable at namespace scope (except for thread_local each thread has own copy ). it differs from a process, which generally does not directly share data with other processes. 
exception thrown in a thread will system.terminate itself and main thread but won’t stop other threads. C++11 has not built-in Thread priority 

Atomic modify data in a single clock tick, impossible for other thread to access the data in the middle of such an operation, memory_order to prevent/control reordering the accesses

For specialize atomic<int> c(0);
i=c.fetch_add|sub(1, memory_order_relaxed); //c++|- -|+|-=1, return contained value before op,, relaxed (for all) only ensure atomicity, no sync nor control over access order: optimizer can reorder when execute

For all atomic<T>
//Atomic store|load move data between shared memory (atomic is for global variable) and thread own memory atomically.

atomic<int> Guard(0);
int Payload = 0;
void prod(){
Payload = 42; //order before
Guard.store(1, memory_order_release); //place a sync point among all threads op on access order to memory of shared variable
release (for store): no reads nor writes in the current thread can be reordered after store. All writes [data dependent on atomic var ] before store in current thread are visible to other threads after acquire [consume] the same atomic var 
}

int main() {  
    thread t(prod);
    t.detach();   while(Guard.load(memory_order_acquire)==0){cout<<"wait"<<endl;}
//acquire (for load): no reads nor writes in the current thread can be reordered before this load.
cout<< Payload<<endl; //order after
    //t.join();
}

Two machine instructions, executed in the same thread, are data-dependent whenever the first outputs a value to a memory location and the second input the value from that memory location. memory ordering among multiple instructions through this dependency chain is preserved

atomic<int*> Guard(nullptr);
int Payload = 0;

Payload = 42;
Guard.store(&Payload, memory_order_release)

g=Guard.load(memory_order_consume;//consume (for load):apply when data dependent on atomic var 
if (g != nullptr)
    p = *g;

acquire | consume is synchronizes-with | dependency-ordered-before relationship 

mach instructions:  cmd output input
lwz r9 O(r8) //load from guard to g
isync //memory barrier for acquire
lwz r10 O(r9) //load from g to p

i=c.exchange(v, memory_acq_rel) : c=v and return previous c
//acq_rel (for read-modify-write op like fetch_add) is both acquire and release, no reads nor writes can be reordered before load | after store. All writes in other threads before release the atomic are visible before this thread modification (by read/acq); this thread modification is visible (by write/rel) to other threads after acquire the atomic 
i=c.compare_exchange_weak|strong(exp,new) : return c==exp if true c=new; false   exp=c; 
weak allow spurious failure (for no reason) return false even when contained==expected, perform better in loop like while(!c.compare_exchange_weak(exp,new) ); won’t set expected to contained in that case
strong requires return true if c==exp not allow spurious fail, preferably for non-loop 

seq_cst (default for all) load (acquire) store (release) read-modify-write (acq_rel)

Volatile instruct compiler no to  optimize: never cache the variable in a register (CPU internal memory to store data being actively processed ), always access the actual memory, never get stale value (memory visibility across threads) but not guarantee  atomicity or synchronization

A Mutex is a locking mechanism while a semaphore is a signalling mechanism. A binary semaphore can be used as a Mutex but a Mutex can never be used as a semaphore

Acquire (mutex object )
Critical Section
Release (mutex);
mutex can be release only by the thread that acquired it, it lets the thread spinlock if the lock is not available: in a loop where the thread periodically checks whether the lock is available. busy waiting wastes CPU cycles, which can be avoided by A condition variable: cv.wait() release the mutex while block the thread then acquire it again upon wake and unblock the thread

condVar.lock.acquire();
while(queue.empty())
{condVar.wait();}//instead of sleep(1);
condVar.lock.release();

condVar.signal() to wake up a random thread that is sitting asleep on syncVar.wait() (or signalAll() to wake up all the waiting threads, which semaphore cannot )

a thread waiting on a semaphore can be signaled by any other thread. Faster than mutex but expensiver
wait(S integer variable)
{while (S<=0);
   S--;}
signal(S)
{S++;}
limitations of semaphores is priority inversion. Low-priority processes enter the critical section, and high-priority processes continue to wait.

A semaphore is better for multiple instances of a resource, but a mutex is better for a single shared resource.
FXP: Route order to the first available avenue, whenever an avenue thread is done processing then semaphore.signal() , wake up a waiting thread to send an order to an avenue 

C++11 don’t have built-in semaphore (C++20 has counting|binary_semaphore), instead  unique_lock<mutex> condition_variable

If the mutex is currently locked by another thread, mutex.lock() thread is blocked until other thread unlock; mutex.trylock() fails and returns false without blocking
if (mtx.try_lock()) {//acquire mtx
      ++counter;
      //throw exception never mtx.unlock, block till system terminate, use try {code throw exception} catch(…){ mtx.unlock(); } (all exception, no finally C++) or 
{ unique_lock|lock_guard<mutex> lc(mtx); code throw exception;} because their de|constructor un|lock mutex, When a scope { … } is exited (by return, reaching the end or throwing an exception) local objects created on stack instead of heap within that scope is destroyed (destructors are called ) : stack unwind

      mtx.unlock();//release mtx
    }

recursive_|mutex does|not allows trylock  and lock from a thread that is already locking it (occur in recursive, not in loop because mutex is unlocked upon exit previous loop), |produces deadlock with undefined behavior 

unique_lock can lock and unlock explicitly instead of immediately locking (by default) on construction and implicit unlock on destruction, it can be moved to transfer lock ownership while lock_guard can not

unique_lock<mutex> lck(mtx,std::defer_lock);
  lck.lock();
  while(!pred()) {cv.wait(lck);}
  cv.wait(lck,pred);
//lock must be acquired by the current thread before wait(), otherwise undefined behavior. predicate is function (access global var) or lambda (access global var and capture local var) without arguments and returns bool, the thread is unblocked when it becomes true (also check against spurious wake-up calls not by notify_one|all). Notifying thread does not need to wait() first nor hold the lock on the same mutex as the waiting thread(s)
  …
  lck.unlock();

lock_guard<mutex> (scoped_lock avoid deadlock C++17) is strict version of unique_lock, will be locked only once on construction and unlocked on destruction (exit scope }), not used with condition_variable
both manage the mutex object and guarantees it is properly unlocked in case an exception is thrown.
unique_lock<mutex> to ensure only 1 thread can access the queue at a time, add condition_variable if threads will wait on item to be stored into the queue by the other thread.

#include <mutex>
once_flag flag;
 void once()
{call_once(flag, lambda|func, para);}
Only one thread is actively executing call_once sharing the same flag, other threads (Passive executions) do not execute but do not return until the active execution itself has returned, if the active throws exception (propagated to its calling thread, handle by try{call_once}catch(…){}), one of passive become active and   execute 

#include <barrier>
barrier init with count of participating threads, wait() to block until all threads arrive(decrement count), having a callback func to be invoke only once by one of threads after the barrier is lifted (count ==0) , after completion callback finished count reset to orig - num of arrive_and_drop, then unblock threads next phase begins (reusable latch + callback)
int main()
{
vector<string> workers{“A”,”B”};
auto on_completion = [](){
  static auto phase = 1;//static remains
  cout<<phase;
  phase++;
};
barrier bar(workers.size(), on_completion);

auto work = [&](string name){
  cout<<name;
  bar.arrive_and_wait(); //phase 1

  cout<<name;
  bar.arrive_and_wait(); phase 2
};

vector<thread> ts;
ts.reserve(size(workers));
for(auto const& worker : workers)
   ts.emplace_back(work,worker);

}
A B 1 A B 2

The arguments to the thread function are moved or copied by value even defined as reference. If a reference argument , usually to return result, needs to be passed to the thread function, it has to be wrapped (e.g., with std::ref or std::cref).
Same to std::bind returns a function object
#include <functional> std::bind / ref
#include <thread>
void inc(int& x)//(vector <int>& v), not (int c)
{x++;}
int main()
{int i= 0;// vector<int> v;
 auto func = bind(inc, i | ref(i)); //ref(v)
  func(); // i = 0 | 1
  thread t([bind(]inc, i | ref(i)));
//lambdas are better than bind in terms of space and speed, also allow to capture by reference or value
thread t( [&i](){ inc(i);} );
thread t(&Mclass::memberfunc, mclass|this, para1, para2);//mclass is an instance, this if the same class
  t.join(); // i = 1 | 2 returns when the thread execution has completed. main don’t finish until all joined threads returned, system terminate if without t.join(), unless t.detach() make t run independently from main, t.joinable() false if already join() or detach()
C++20 jthread (joining thread) auto join upon destruct if joinable, also can request_stop() via stop_token to function 
}

void foo(stop_token st){
  if (st.stop_requested()){return;}
…
}
jthread jt(foo); //always has stop token regardless foo signature, specify to be referred in function 
//stop_token s = jt.get_stop_token();
jt.request_stop();
jt.join();

double divide (double x, double y) {return x/y;}
 int main()
{ using namespace std::placeholders;
  auto invert = bind(divide,_2,_1);            
  cout << invert(10,2); //0.2 2/10                  
  auto round = bind<int>(divide,_1,_2); 
  std::cout << round(10,3); //3 int(10/3)

By default Any return value from the thread function is ignored. If the function throws an exception, std::terminate is called.
std::future to transmit value and exception from thread func to calling thread 
promise.set_value(ret) | set_exception(ex) > future=promise.get_future() > future.get() from another thread

#include <functional>     // std::ref
#include <thread>         // std::thread
#include <future>         // std::promise, std::future
#include <exception>      // std::exception, std::current_exception

void set (std::promise<int>& prom) {
  int x;
  try {prom.set_value(x);}
  catch (std::exception&) {
prom.set_exception(std::current_exception());//use shared pointer to the exception 
  }
}
void get (std::future<int>& fut) {
  try {
    int x = fut.get();
  }
  catch (std::exception& e) {
    cout << e.what();//set by prom
  }
}
int main ()
{
  std::promise<int> prom;
  std::future<int> fut = prom.get_future();
  std::thread th1 (get, std::ref(fut));
  std::thread th2 (set, std::ref(prom));
  th1.join();
  th2.join();
  return 0;
}

vector<int> v1;
//functor, must auto because it is compiler-generated unknown type
 auto f= [&] (int m)// access v1 by ref
    {
        v1.push_back(m);
    };
 f(3);
//unnamed function 
    [v1]() // access v1 by copy value 
    {
        for (auto p = v1.begin(); p != v1.end(); p++)
        {cout << *p << " ";}
    };

//concise and readable such as callback functions
std::thread t( [](){ cout << std::this_thread::get_id();} );//get id inside thread func
thread t = std::thread{[this]() { this->proc(); }};//class member proc
thread t([](int i){…},2);
thread t(bind(foo,_1),2); //foo(int i){…}
//cout<<t.get_id();//get id outside 

capture when the lambda is created whereas passing argument when the lambda is called, and captured value can be remembered if mutable (only applicable to value capture , change on reference capture in lambda reflects back to main function): each call to the lambda could cause a change in the lambda itself, has no thing to do with the actual value of x in the main function

    int x = 5;
    int y;
    auto lamb = [x](int i) mutable {return x++ + i; };//if no mutable: compile error can not assign to immutable captured value
    y= lamb(5);
    cout << y<<","<< x << endl; //outputs 10,5
    x = 20;
    y = lamb(10);
    cout << y << "," << x << endl; //outputs 16,20
}

f(p1,p2,p3) f1 is a "partial application" of f:
vector<int> largeData;//copy STL containers into a closure is very expensive, so prefer moving 
auto f1 = std::bind(f, 4, [std::placeholders::]_1, std::move(largeData) | a+b);
Unlike 14 below, C++11 lambda can’t capture rvalue(move or expression), nor perfect forwarding, 
auto f1 = [largeData](auto arg) { f(4, arg, std::move(largeData)); };
f1(5); //f(4,5,v); 1st arg in f1 map to 2nd arg in f

std::function is generalized function pointers and std::bind provide forwarding to a callable to implement callback mechanism: don't want to wait for long run function to return, then you can run that function on separate thread and give it a function pointer that it will callback after it completes.
#include <functional>
class MyClass {
private:
    //just shorthand to avoid long typing
    typedef std::function<void (int)> TCallback;
    void longRun(TCallback callback)
    {
        int ret = svcCall();
        callback(ret);
    }
    void completeCallback(int result)
    {std::cout << result;}
public:
    int longRunAsync()
    {auto callback = std::bind(&MyClass::completeCallback, 
            this, std::placeholders::_1);
//& is same as no &, this (&myClass) is pointer to current instance for member function 
//equivalent lambda 
std::function<void (int)> callback = [](int result) { std::cout << result;};
        std::thread t(&|longRun, callback);
        t.join();
    }
};

void/generic pointer void* to some memory that the compiler doesn't know the type of. + that is why  
void *operator new(size_t); returns a pointer to unknown type void *p = &t

A shared pointer has one pointer to the object and one pointer to a control block (holding the ref counter, links to weak pointers ...).
Even reference counter is thread-safe by atomic in/decrement, std::shared_ptr itself is NOT thread-safe: access to data object requires mutexes or synchronization among threads 
So C++14 has
//In thread 1
shared_ptr<myClass> private = atomic_load(&global);
//In thread 2
atomic_store(&global, make_shared<myClass>());
C++17 has atomic<shared_ptr<myClass>> global;
accesses to global will be thread safe

//as replacement of auto pointer, unique pointer and associated object are deleted together 
struct A{
  A(string n):Data(n){}
  string Data;
    ~A(){cout<<"Deleted.";}
};
void PassIn(std::unique_ptr<A> a)
{
    cout<< "Pointer received."<<'\n';
} // a and its object are both deleted.
int main(){ 
    unique_ptr<A> a[(new A(“dt”))]; //empty (null pointer) without[ ]
    a = new A(“dt”));//error 
    if (a) // (bool)false if not associated.
         cout<<*a; 
    auto x = make_unique<A>(“dt”);
    auto y = *x; // dereference pointer
    auto r = x.get();// get the internal raw pointer, do NOT delete it since managed 
    c->Data = "td "; // access members
    auto z = x; // Error: the object belongs to unique pointer
    auto z = move(x); //z owns the object and x null pointer
    x = make_unique<A>(“dd”);//~       x.reset(new A(“dd”)/nullptr); prev object deleted 
    swap(x,z);
    //use move() passing by value and transfer ownership to pointer a in the target function, or directly passing it by ref to void PassIn(std::unique_ptr<A>& a) without ownership transfer
    PassIn(move(x)) // Pointer received.
    ; // Deleted.
   if (!x) cout<< "x is empty."; // true: x is empty.


Linux
kill -9|3 <pid> to terminate | take A thread dump provides a snapshot of all the threads in a program

ulimit -S -c unlimited|0 //en|disable core dump
permanently by /etc/security/limits.conf
* soft core unlimited|0 //override per user
* hard core 0 //unoverridable
ulimit -c //check denote the setting

sudo sysctl -w kernel.core_pattern=/coredumps/core-%e-%s-%u-%g-%p-%t
to update core_pattern file /proc/sys/kernel/core_pattern from default location pattern /usr/libexec/abrt-hook-ccpp %s %c %p %u %g %t e %P %I
permanently by  /etc/sysctl.conf
kernel.core_pattern="/coredumps/core-%e-%s-%u-%g-%p-%t"

%e executable name // ./myapp generate /coredumps/core-myapp upon crash by an unhandled exception 
%g GID of dumped process
%h  Hostname
%i  TID of thread that triggered core dump
%p  PID of dumped process
%s  Number of signal causing dump
%u  UID of dumped process

VWAP try to reach the best price given time window (fxp 6hrs) , use price trail per sec/min/hr to predict direction, when price moving in favor, hold until it slow down which means it is getting close to the peak/bottom where the trade should be made to achieve the best price (usually not by limited price). the whole order can be divided into sub time window (fxp 1hr), instead of waiting and missing execution. If missed a sub window, volumes of the rest window can be adjusted accordingly to match up. IS metrics to evaluate VWAP performance 
